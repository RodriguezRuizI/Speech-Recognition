{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLAS_Xvector_model_Prosody_with_DataAugmentation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnC9nFpl_uP_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eaf191e-fcc6-4c33-e120-d3dbbb8badc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n",
        "from keras import Sequential, Input, Model\n",
        "from keras.layers import Conv1D, BatchNormalization, Dropout, Dense, Softmax, ReLU, Lambda, Activation\n",
        "from keras import optimizers\n",
        "\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import numpy as np\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect Google Colab to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXspSNWpHXxt",
        "outputId": "9d431040-02d1-4d02-9d05-42091545b16f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load data into Pandas dataset**"
      ],
      "metadata": {
        "id": "ed9DcgobRk3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PATH = '/content/gdrive/My Drive/Gita/RositaNorm/'\n",
        "dataset_file = TRAIN_PATH + 'labels.csv'"
      ],
      "metadata": {
        "id": "vFQVZEo6Hk-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(dataset_file)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "4e3spzBqIPf1",
        "outputId": "e599e623-68fa-4f86-df07-d7d4d1c19485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Unnamed: 0                   Filename  Gender  Disease\n",
              "0            0  AVPEPUDEAC0001_rosita.wav     NaN        0\n",
              "1            1  AVPEPUDEAC0003_rosita.wav     NaN        0\n",
              "2            2  AVPEPUDEAC0004_rosita.wav     NaN        0\n",
              "3            3  AVPEPUDEAC0005_rosita.wav     NaN        0\n",
              "4            4  AVPEPUDEAC0006_rosita.wav     NaN        0\n",
              "..         ...                        ...     ...      ...\n",
              "95          95   AVPEPUDEA0055_rosita.wav     NaN        1\n",
              "96          96   AVPEPUDEA0056_rosita.wav     NaN        1\n",
              "97          97   AVPEPUDEA0057_rosita.wav     NaN        1\n",
              "98          98   AVPEPUDEA0058_rosita.wav     NaN        1\n",
              "99          99   AVPEPUDEA0059_rosita.wav     NaN        1\n",
              "\n",
              "[100 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-270a301b-69e4-49fe-b68b-ade8afb4e69a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Filename</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Disease</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>AVPEPUDEAC0001_rosita.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>AVPEPUDEAC0003_rosita.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>AVPEPUDEAC0004_rosita.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>AVPEPUDEAC0005_rosita.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>AVPEPUDEAC0006_rosita.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>95</td>\n",
              "      <td>AVPEPUDEA0055_rosita.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>96</td>\n",
              "      <td>AVPEPUDEA0056_rosita.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>97</td>\n",
              "      <td>AVPEPUDEA0057_rosita.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>98</td>\n",
              "      <td>AVPEPUDEA0058_rosita.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>99</td>\n",
              "      <td>AVPEPUDEA0059_rosita.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows Ã— 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-270a301b-69e4-49fe-b68b-ade8afb4e69a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-270a301b-69e4-49fe-b68b-ade8afb4e69a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-270a301b-69e4-49fe-b68b-ade8afb4e69a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(data[\"Filename\"].to_numpy(), data[\"Disease\"].to_numpy(), test_size= 0.3, random_state=True, shuffle=True)\n",
        "#df_X_train, df_X_val, df_y_train, df_y_val = train_test_split(df_X_train, df_y_train, test_size=0.2, random_state=True, shuffle=True)"
      ],
      "metadata": {
        "id": "5odmPDl_Qu-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Fn4C78Hvk7g",
        "outputId": "93d69c86-34f7-4640-902d-3412e46b4dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['AVPEPUDEA0022_rosita.wav', 'AVPEPUDEA0059_rosita.wav',\n",
              "       'AVPEPUDEA0006_rosita.wav', 'AVPEPUDEA0055_rosita.wav',\n",
              "       'AVPEPUDEA0048_rosita.wav', 'AVPEPUDEAC0046_rosita.wav',\n",
              "       'AVPEPUDEAC0054_rosita.wav', 'AVPEPUDEA0011_rosita.wav',\n",
              "       'AVPEPUDEAC0026_rosita.wav', 'AVPEPUDEAC0040_rosita.wav',\n",
              "       'AVPEPUDEA0046_rosita.wav', 'AVPEPUDEA0005_rosita.wav',\n",
              "       'AVPEPUDEA0034_rosita.wav', 'AVPEPUDEAC0018_rosita.wav',\n",
              "       'AVPEPUDEA0042_rosita.wav', 'AVPEPUDEAC0047_rosita.wav',\n",
              "       'AVPEPUDEAC0051_rosita.wav', 'AVPEPUDEA0051_rosita.wav',\n",
              "       'AVPEPUDEAC0029_rosita.wav', 'AVPEPUDEA0058_rosita.wav',\n",
              "       'AVPEPUDEAC0049_rosita.wav', 'AVPEPUDEA0007_rosita.wav',\n",
              "       'AVPEPUDEAC0027_rosita.wav', 'AVPEPUDEAC0006_rosita.wav',\n",
              "       'AVPEPUDEA0010_rosita.wav', 'AVPEPUDEAC0057_rosita.wav',\n",
              "       'AVPEPUDEAC0024_rosita.wav', 'AVPEPUDEA0047_rosita.wav',\n",
              "       'AVPEPUDEAC0005_rosita.wav', 'AVPEPUDEA0030_rosita.wav',\n",
              "       'AVPEPUDEAC0034_rosita.wav', 'AVPEPUDEA0021_rosita.wav',\n",
              "       'AVPEPUDEA0025_rosita.wav', 'AVPEPUDEAC0048_rosita.wav',\n",
              "       'AVPEPUDEAC0053_rosita.wav', 'AVPEPUDEA0049_rosita.wav',\n",
              "       'AVPEPUDEAC0011_rosita.wav', 'AVPEPUDEA0013_rosita.wav',\n",
              "       'AVPEPUDEAC0001_rosita.wav', 'AVPEPUDEA0050_rosita.wav',\n",
              "       'AVPEPUDEA0009_rosita.wav', 'AVPEPUDEAC0025_rosita.wav',\n",
              "       'AVPEPUDEA0014_rosita.wav', 'AVPEPUDEA0016_rosita.wav',\n",
              "       'AVPEPUDEAC0010_rosita.wav', 'AVPEPUDEA0056_rosita.wav',\n",
              "       'AVPEPUDEAC0016_rosita.wav', 'AVPEPUDEA0023_rosita.wav',\n",
              "       'AVPEPUDEA0045_rosita.wav', 'AVPEPUDEAC0017_rosita.wav',\n",
              "       'AVPEPUDEAC0033_rosita.wav', 'AVPEPUDEAC0031_rosita.wav',\n",
              "       'AVPEPUDEAC0014_rosita.wav', 'AVPEPUDEAC0021_rosita.wav',\n",
              "       'AVPEPUDEAC0023_rosita.wav', 'AVPEPUDEA0001_rosita.wav',\n",
              "       'AVPEPUDEAC0028_rosita.wav', 'AVPEPUDEAC0008_rosita.wav',\n",
              "       'AVPEPUDEA0026_rosita.wav', 'AVPEPUDEA0032_rosita.wav',\n",
              "       'AVPEPUDEAC0003_rosita.wav', 'AVPEPUDEAC0019_rosita.wav',\n",
              "       'AVPEPUDEA0017_rosita.wav', 'AVPEPUDEA0037_rosita.wav',\n",
              "       'AVPEPUDEAC0007_rosita.wav', 'AVPEPUDEA0031_rosita.wav',\n",
              "       'AVPEPUDEAC0012_rosita.wav', 'AVPEPUDEA0027_rosita.wav',\n",
              "       'AVPEPUDEAC0015_rosita.wav', 'AVPEPUDEAC0043_rosita.wav'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv7Hr2lReIcs",
        "outputId": "b3a8e3ab-2922-4807-d30c-b29f0b938d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
              "       0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
              "       0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
              "       0, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Perform Data Augmentation**"
      ],
      "metadata": {
        "id": "lA0e_pb3Z8U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install audiomentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSHhHcb9awUv",
        "outputId": "08ccaccd-08d0-4938-e776-f197a3c1af98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting audiomentations\n",
            "  Downloading audiomentations-0.24.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (1.21.6)\n",
            "Requirement already satisfied: scipy<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (1.4.1)\n",
            "Requirement already satisfied: librosa<0.10.0,>0.7.2 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (0.8.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (1.0.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (1.6.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (0.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (21.3)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (1.1.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (2.1.9)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (0.10.3.post1)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (0.51.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa<0.10.0,>0.7.2->audiomentations) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa<0.10.0,>0.7.2->audiomentations) (0.34.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa<0.10.0,>0.7.2->audiomentations) (3.0.9)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (1.4.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (1.24.3)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa<0.10.0,>0.7.2->audiomentations) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa<0.10.0,>0.7.2->audiomentations) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa<0.10.0,>0.7.2->audiomentations) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa<0.10.0,>0.7.2->audiomentations) (2.21)\n",
            "Installing collected packages: audiomentations\n",
            "Successfully installed audiomentations-0.24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation was applied only to the audios in the training split, and not to the ones in the test split. These transformations include Noise Addition, Pitch Scaling (change of the pitch of the voice), Time Stretching (changing the speed of the sound but without changing the pitch) and Polarity Inversion (multiply the waveform by -1)."
      ],
      "metadata": {
        "id": "nxhYzCDvNSFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from audiomentations import Compose, AddGaussianNoise, PitchShift, TimeStretch, PolarityInversion, Normalize, HighPassFilter\n",
        "\n",
        "NUM_AUGMENTATIONS = 10 #number of augmentations per training signal\n",
        "\n",
        "augment = Compose([\n",
        "  AddGaussianNoise(min_amplitude=0.1, max_amplitude=0.2, p=0.5),\n",
        "  PitchShift(min_semitones=-4, max_semitones=4, p=0.7),\n",
        "  TimeStretch(min_rate=0.8, max_rate=1.25, p=0.7),\n",
        "  PolarityInversion(p=0.7),\n",
        "])\n"
      ],
      "metadata": {
        "id": "CCJHi9VcZ7ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Obtain MFCCs and create train and test data subsets with them**"
      ],
      "metadata": {
        "id": "veH2dSqLRqOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_RATE = 44100\n",
        "NUM_MFCC = 40\n",
        "MFCC_MAX_LEN = 500"
      ],
      "metadata": {
        "id": "Ee3UivC3LLmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_audio(audio):\n",
        "    audio = audio / np.max(np.abs(audio))\n",
        "    return audio"
      ],
      "metadata": {
        "id": "WsdOwAt_lua5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to extract MFCCs from audio signal"
      ],
      "metadata": {
        "id": "ds8NpL0fLFhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def audio_to_mfcc(audio, max_len=MFCC_MAX_LEN):\n",
        "\n",
        "    audio = normalize_audio(audio)\n",
        "    \n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=SAMPLE_RATE, n_mfcc=NUM_MFCC)\n",
        "\n",
        "    # If maximum length exceeds mfcc lengths then pad the remaining ones\n",
        "    if (max_len > mfcc.shape[1]):\n",
        "        pad_width = max_len - mfcc.shape[1]\n",
        "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "\n",
        "    # Else cutoff the remaining parts\n",
        "    else:\n",
        "        mfcc = mfcc[:, :max_len]\n",
        "    \n",
        "    return mfcc"
      ],
      "metadata": {
        "id": "XtfpXpXHUfl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def append_X_Y(X, y, label, audio):\n",
        "    y.append(label)\n",
        "    mfcc = audio_to_mfcc(audio)\n",
        "    X.append(mfcc)"
      ],
      "metadata": {
        "id": "VOFlsnbYIQOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create new training dataset with the MFCC coefficients obtained from the original and augmented audios"
      ],
      "metadata": {
        "id": "_Vk4wfLtLLbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "PD_idx = 0\n",
        "HC_idx = 0\n",
        "\n",
        "for idx, audio_filename in tqdm(enumerate(df_X_train)):\n",
        "    label = df_y_train[idx]\n",
        "    audio, sr = librosa.load(TRAIN_PATH + audio_filename, sr=44100)\n",
        "    \n",
        "    if (label == 1):\n",
        "      PD_idx = idx\n",
        "    else:\n",
        "      HC_idx = idx\n",
        "\n",
        "    append_X_Y(X_train, y_train, label, audio)\n",
        "\n",
        "    # Perform data augmentation if NUM_AUGMENTATIONS > 0\n",
        "    for i in range(NUM_AUGMENTATIONS):\n",
        "      augmented_audio = augment(audio, sr)\n",
        "      append_X_Y(X_train, y_train, label, augmented_audio)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ex_Ks4cIQMT",
        "outputId": "5f85dc76-bb01-4a16-fea9-9594d9804fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "70it [04:52,  4.18s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_train.shape[0] == len(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5VBfetjIQJ3",
        "outputId": "171add05-99f5-4cb8-aea3-0586b95e307c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQfrTEKkv-wy",
        "outputId": "7e512fec-6af6-4527-ee8c-47c6711e848e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDTecn3YdyYE",
        "outputId": "c6cec16d-60c1-48ea-e023-644dab47f72a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random shuffle of the new dataset\n",
        "X_train, y_train = sklearn.utils.shuffle(X_train, y_train)"
      ],
      "metadata": {
        "id": "2G9qwXF4soOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLEa584Us08V",
        "outputId": "504a5438-c00b-4115-f04d-311311cf4690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
              "       1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
              "       1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
              "       1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
              "       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
              "       0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
              "       1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
              "       0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "       1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
              "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
              "       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
              "       1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
              "       1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
              "       1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
              "       1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
              "       1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
              "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
              "       1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
              "       1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
              "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
              "       0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
              "       1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
              "       1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
              "       0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#X_val = []\n",
        "#y_val = []\n",
        "\n",
        "#for idx, audio_filename in tqdm(enumerate(df_X_val)):\n",
        "#    label = df_y_val[idx]\n",
        "#    audio, sr = librosa.load(TRAIN_PATH + audio_filename, sr=44100)\n",
        "    \n",
        "#    append_X_Y(X_val, y_val, label, audio)"
      ],
      "metadata": {
        "id": "BirA-Bz1uH3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_val = np.array(X_val)\n",
        "#y_val = np.array(y_val)\n",
        "#X_val.shape[0] == len(y_val)"
      ],
      "metadata": {
        "id": "nPiTROmourue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create new test dataset with the MFCC coefficients obtained from the original audios"
      ],
      "metadata": {
        "id": "oUFedNzbLmbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "for idx, audio_filename in tqdm(enumerate(df_X_test)):\n",
        "    label = df_y_test[idx]\n",
        "    audio, sr = librosa.load(TRAIN_PATH + audio_filename, sr=44100)\n",
        "    \n",
        "    append_X_Y(X_test, y_test, label, audio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8owpUchuh5f",
        "outputId": "2d79cbe3-c984-4d2b-ca89-12e251a5b665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "30it [00:12,  2.45it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "X_test.shape[0] == len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqwJ3F0CuxB2",
        "outputId": "639332d9-79af-4341-9293-243adec0de61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whXgqZ1PVVl-",
        "outputId": "adc2dc7b-871e-4678-c09a-c969a4215a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Build TDNN as a sequential model**"
      ],
      "metadata": {
        "id": "vWBwPbc9R0V-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_dim_1 = NUM_MFCC  #number of MFCCs\n",
        "feature_dim_2 = MFCC_MAX_LEN  #max length for the MFCCs"
      ],
      "metadata": {
        "id": "B6guoCbNYCIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TDNN model\n",
        "numFilters = 32\n",
        "dropout_rate = 0.2\n"
      ],
      "metadata": {
        "id": "aD9Vdbd5xBlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer 1: Input\n",
        "input_tensor = Input(shape=(feature_dim_1, feature_dim_2))\n",
        "\n",
        "# Layer 2\n",
        "x = Conv1D(numFilters, 5, dilation_rate=1)(input_tensor)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(dropout_rate)(x)\n",
        "x = ReLU()(x)\n",
        "# Layer 3\n",
        "x = Conv1D(numFilters, 3, dilation_rate=2)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(dropout_rate)(x)\n",
        "x = ReLU()(x)\n",
        "# Layer 4\n",
        "x = Conv1D(numFilters, 3, dilation_rate=3)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(dropout_rate)(x)\n",
        "x = ReLU()(x)\n",
        "# Layer 5\n",
        "x = Conv1D(numFilters, 1, dilation_rate=1)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(dropout_rate)(x)\n",
        "x = ReLU()(x)\n",
        "# Layer 6\n",
        "x = Conv1D(1500, 1, dilation_rate=1)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(dropout_rate)(x)\n",
        "x = ReLU()(x)\n",
        "\n",
        "# Layer 7: stats pooling\n",
        "mean = tf.math.reduce_mean(x, axis=1)\n",
        "std = tf.math.reduce_variance(x, axis=1)\n",
        "stat_pooling = tf.concat((mean, std), axis=1)\n",
        "x_vector = Activation('linear')(stat_pooling)  #x-vectors\n",
        "\n",
        "x_vec_model = Model(inputs = input_tensor, outputs = x_vector);"
      ],
      "metadata": {
        "id": "JTOF6MwvKzzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_vec_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M61S6KS_OcnO",
        "outputId": "975cb040-f395-4675-f644-32d1b727b507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 40, 500)]    0           []                               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 36, 32)       80032       ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 36, 32)      128         ['conv1d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 36, 32)       0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " re_lu (ReLU)                   (None, 36, 32)       0           ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 32, 32)       3104        ['re_lu[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 32, 32)      128         ['conv1d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 32, 32)       0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " re_lu_1 (ReLU)                 (None, 32, 32)       0           ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 26, 32)       3104        ['re_lu_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 26, 32)      128         ['conv1d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 26, 32)       0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " re_lu_2 (ReLU)                 (None, 26, 32)       0           ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 26, 32)       1056        ['re_lu_2[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 26, 32)      128         ['conv1d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 26, 32)       0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " re_lu_3 (ReLU)                 (None, 26, 32)       0           ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 26, 1500)     49500       ['re_lu_3[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 26, 1500)    6000        ['conv1d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 26, 1500)     0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " re_lu_4 (ReLU)                 (None, 26, 1500)     0           ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean (TFOpLambd  (None, 1500)        0           ['re_lu_4[0][0]']                \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.reduce_variance (TFOpL  (None, 1500)        0           ['re_lu_4[0][0]']                \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " tf.concat (TFOpLambda)         (None, 3000)         0           ['tf.math.reduce_mean[0][0]',    \n",
            "                                                                  'tf.math.reduce_variance[0][0]']\n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 3000)         0           ['tf.concat[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 143,308\n",
            "Trainable params: 140,052\n",
            "Non-trainable params: 3,256\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUaq8hUSGDKR",
        "outputId": "6767f5bb-1d5c-422d-b309-c8e88aa26fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The x-vectors are extracted for each of the test samples. These x-vectors will be the input to the LDA (or PLDA) model, which will be trained to fit the training data. The TDNN model is not trained as there aren't any dense layers (the model works only as and embedder to extract the x-vectors)."
      ],
      "metadata": {
        "id": "ix8TwORMLz4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_vectors_training = x_vec_model.predict(X_train)"
      ],
      "metadata": {
        "id": "MLeijDV9E2-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_vectors_training.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "919ynnP7FJZr",
        "outputId": "98e94a8e-206f-4a8a-a2a1-f8219180e1e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(770, 3000)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#x_vectors_val = x_vec_model.predict(X_val)"
      ],
      "metadata": {
        "id": "7Mtv-voUG2Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#x_vectors_val.shape"
      ],
      "metadata": {
        "id": "7oknER0jG8ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_vectors_test = x_vec_model.predict(X_test)"
      ],
      "metadata": {
        "id": "TqTI5UOOG_HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_vectors_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBuAiF_THDj1",
        "outputId": "04cd00b0-f96b-4517-fcc7-c6cc8c20c888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 3000)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV6ype7cEKp_",
        "outputId": "ebbc0f01-e23d-427e-dfcc-d58f62c1897a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikeras\n",
            "  Downloading scikeras-0.8.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: importlib-metadata>=3 in /usr/local/lib/python3.7/dist-packages (from scikeras) (4.11.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikeras) (1.0.2)\n",
            "Requirement already satisfied: packaging<22.0,>=0.21 in /usr/local/lib/python3.7/dist-packages (from scikeras) (21.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3->scikeras) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3->scikeras) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging<22.0,>=0.21->scikeras) (3.0.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.4.1)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train LDA model**"
      ],
      "metadata": {
        "id": "Pwk10JKvZMJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of this model will be the classifications of the audios (1 for PD and 0 for HC) embedded as x-vectors. "
      ],
      "metadata": {
        "id": "64CzaTIzMl_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_validate, cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#epochs = 20\n",
        "#batch_size = 5\n",
        "\n",
        "# LDA model definition\n",
        "LDA_model = LinearDiscriminantAnalysis()\n",
        "\n",
        "# Create pipeline\n",
        "#pipeline = Pipeline(steps=[('x_vec_model',x_vec),('LDA_model',LDA_model)])\n",
        "\n",
        "cv_results = cross_validate(LDA_model, x_vectors_training, y_train, cv=5, \n",
        "                            scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\", \"roc_auc\"], return_train_score=True, verbose=1)\n",
        "\n",
        "print(\"\\nValidation accuracy: {mean_accuracy} +/- {std_accuracy}\".format(\n",
        "    mean_accuracy=np.mean(cv_results['test_accuracy']),\n",
        "    std_accuracy=np.std(cv_results['test_accuracy'])))\n",
        "print(\"Training accuracy: {mean_accuracy} +/- {std_accuracy}\".format(\n",
        "    mean_accuracy=np.mean(cv_results['train_accuracy']),\n",
        "    std_accuracy=np.std(cv_results['train_accuracy'])))\n",
        "print(\"Validation recall: {mean_recall} +/- {std_recall}\".format(\n",
        "    mean_recall=np.mean(cv_results['test_recall_macro']),\n",
        "    std_recall=np.std(cv_results['test_recall_macro'])))\n",
        "print(\"Training recall: {mean_recall} +/- {std_recall}\".format(\n",
        "    mean_recall=np.mean(cv_results['train_recall_macro']),\n",
        "    std_recall=np.std(cv_results['train_recall_macro'])))\n",
        "print(\"Validation precision: {mean_precision} +/- {std_precision}\".format(\n",
        "    mean_precision=np.mean(cv_results['test_precision_macro']),\n",
        "    std_precision=np.std(cv_results['test_precision_macro'])))\n",
        "print(\"Training precision: {mean_precision} +/- {std_precision} \\n\".format(\n",
        "    mean_precision=np.mean(cv_results['train_precision_macro']),\n",
        "    std_precision=np.std(cv_results['train_precision_macro'])))\n",
        "\n",
        "print(\"Validation ROC AUC: {mean_auc} +/- {std_auc}\".format(\n",
        "    mean_auc=np.mean(cv_results['test_roc_auc']),\n",
        "    std_auc=np.std(cv_results['test_roc_auc'])))\n",
        "print(\"Training ROC AUC: {mean_auc} +/- {std_auc} \\n\".format(\n",
        "    mean_auc=np.mean(cv_results['train_roc_auc']),\n",
        "    std_auc=np.std(cv_results['train_roc_auc'])))\n",
        "\n",
        "print(\"Fit mean time: {fit_time}\".format(fit_time=np.mean(cv_results['fit_time'])))\n",
        "print(\"Score mean time: {score_time} \\n\".format(score_time=np.mean(cv_results['score_time'])))\n",
        "\n",
        "#LDA_acc = np.mean(cv_results['test_accuracy'])\n",
        "#LDA_auc = np.mean(cv_results['test_roc_auc'])\n",
        "\n",
        "# Confusion matrix\n",
        "y_pred = cross_val_predict(LDA_model, x_vectors_test, y_test, cv=5)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm_results = {'tn': cm[0, 0], 'fp': cm[0, 1], 'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"True Positives: {tp}\".format(tp=cm_results['tp']))\n",
        "print(\"False Positives: {fp}\".format(fp=cm_results['fp']))\n",
        "print(\"True Negatives: {tn}\".format(tn=cm_results['tn']))\n",
        "print(\"False Negatives: {fn}\\n\".format(fn=cm_results['fn']))\n",
        "\n",
        "test_acc = (cm_results['tp']+cm_results['tn'])/(cm_results['tp']+cm_results['fp']+cm_results['tn']+cm_results['fn'])\n",
        "test_precision = cm_results['tp']/(cm_results['tp']+cm_results['fp'])\n",
        "test_recall = cm_results['tp']/(cm_results['tp']+cm_results['fn'])\n",
        "\n",
        "print(\"Test accuracy: {}\".format(test_acc))\n",
        "print(\"Test precision: {}\".format(test_precision))\n",
        "print(\"Test recall: {}\".format(test_recall))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1odTqyh7TAz",
        "outputId": "12808c26-cab9-4078-f07a-39d13ab005b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    6.7s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation accuracy: 0.6064935064935064 +/- 0.01995102791654187\n",
            "Training accuracy: 0.9581168831168831 +/- 0.006194410398811342\n",
            "Validation recall: 0.6063311095906033 +/- 0.02062456175465307\n",
            "Training recall: 0.9580395880067332 +/- 0.006253509962032595\n",
            "Validation precision: 0.6067161007882844 +/- 0.02048359708058242\n",
            "Training precision: 0.9581493949025086 +/- 0.006150986933280926 \n",
            "\n",
            "Validation ROC AUC: 0.6707579256471663 +/- 0.030610682396348978\n",
            "Training ROC AUC: 0.982313808855235 +/- 0.003405844001211975 \n",
            "\n",
            "Fit mean time: 1.2902331829071045\n",
            "Score mean time: 0.015572452545166015 \n",
            "\n",
            "Confusion Matrix:\n",
            "True Positives: 9\n",
            "False Positives: 5\n",
            "True Negatives: 9\n",
            "False Negatives: 7\n",
            "\n",
            "Test accuracy: 0.6\n",
            "Test precision: 0.6428571428571429\n",
            "Test recall: 0.5625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute only for cosine similarity measure (finally not applied)"
      ],
      "metadata": {
        "id": "2otO9dOO7Nxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3BAxFB9L7G3v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}