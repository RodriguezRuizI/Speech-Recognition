{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLAS_Xvector_model_with_DataAugmentation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {
        "id": "YnC9nFpl_uP_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf75aea7-a3f2-4058-98dd-6d36be4f557b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n",
        "from keras import Sequential, Input, Model\n",
        "from keras.layers import Conv1D, BatchNormalization, Dropout, Dense, Softmax, ReLU, Lambda, Activation\n",
        "from keras import optimizers\n",
        "\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import numpy as np\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect Google Colab to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXspSNWpHXxt",
        "outputId": "5f75cf2d-dcd4-4dc1-85c6-252d82ef9a0b"
      },
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load data into Pandas dataset**"
      ],
      "metadata": {
        "id": "ed9DcgobRk3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PATH = '/content/gdrive/My Drive/Gita/Pataka/'\n",
        "dataset_file = TRAIN_PATH + 'labels.csv'"
      ],
      "metadata": {
        "id": "vFQVZEo6Hk-8"
      },
      "execution_count": 398,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(dataset_file)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "4e3spzBqIPf1",
        "outputId": "33ed3eab-bcdb-4431-ff93-de3280ecf94b"
      },
      "execution_count": 399,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Unnamed: 0                   Filename  Gender  Disease\n",
              "0            0  AVPEPUDEAC0001_pataka.wav     NaN        0\n",
              "1            1  AVPEPUDEAC0003_pataka.wav     NaN        0\n",
              "2            2  AVPEPUDEAC0004_pataka.wav     NaN        0\n",
              "3            3  AVPEPUDEAC0005_pataka.wav     NaN        0\n",
              "4            4  AVPEPUDEAC0006_pataka.wav     NaN        0\n",
              "..         ...                        ...     ...      ...\n",
              "95          95   AVPEPUDEA0055_pataka.wav     NaN        1\n",
              "96          96   AVPEPUDEA0056_pataka.wav     NaN        1\n",
              "97          97   AVPEPUDEA0057_pataka.wav     NaN        1\n",
              "98          98   AVPEPUDEA0058_pataka.wav     NaN        1\n",
              "99          99   AVPEPUDEA0059_pataka.wav     NaN        1\n",
              "\n",
              "[100 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fa7602bd-0660-4d3f-b19b-ba593f9fdcf6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Filename</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Disease</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>AVPEPUDEAC0001_pataka.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>AVPEPUDEAC0003_pataka.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>AVPEPUDEAC0004_pataka.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>AVPEPUDEAC0005_pataka.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>AVPEPUDEAC0006_pataka.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>95</td>\n",
              "      <td>AVPEPUDEA0055_pataka.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>96</td>\n",
              "      <td>AVPEPUDEA0056_pataka.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>97</td>\n",
              "      <td>AVPEPUDEA0057_pataka.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>98</td>\n",
              "      <td>AVPEPUDEA0058_pataka.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>99</td>\n",
              "      <td>AVPEPUDEA0059_pataka.wav</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows Ã— 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa7602bd-0660-4d3f-b19b-ba593f9fdcf6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fa7602bd-0660-4d3f-b19b-ba593f9fdcf6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fa7602bd-0660-4d3f-b19b-ba593f9fdcf6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 399
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(data[\"Filename\"].to_numpy(), data[\"Disease\"].to_numpy(), test_size= 0.3, random_state=True, shuffle=True)\n",
        "#df_X_train, df_X_val, df_y_train, df_y_val = train_test_split(df_X_train, df_y_train, test_size=0.2, random_state=True, shuffle=True)"
      ],
      "metadata": {
        "id": "5odmPDl_Qu-o"
      },
      "execution_count": 400,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Fn4C78Hvk7g",
        "outputId": "c7d60672-ea50-418b-d76c-ede9cbae8173"
      },
      "execution_count": 401,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['AVPEPUDEA0022_pataka.wav', 'AVPEPUDEA0059_pataka.wav',\n",
              "       'AVPEPUDEA0006_pataka.wav', 'AVPEPUDEA0055_pataka.wav',\n",
              "       'AVPEPUDEA0048_pataka.wav', 'AVPEPUDEAC0046_pataka.wav',\n",
              "       'AVPEPUDEAC0054_pataka.wav', 'AVPEPUDEA0011_pataka.wav',\n",
              "       'AVPEPUDEAC0026_pataka.wav', 'AVPEPUDEAC0040_pataka.wav',\n",
              "       'AVPEPUDEA0046_pataka.wav', 'AVPEPUDEA0005_pataka.wav',\n",
              "       'AVPEPUDEA0034_pataka.wav', 'AVPEPUDEAC0018_pataka.wav',\n",
              "       'AVPEPUDEA0042_pataka.wav', 'AVPEPUDEAC0047_pataka.wav',\n",
              "       'AVPEPUDEAC0051_pataka.wav', 'AVPEPUDEA0051_pataka.wav',\n",
              "       'AVPEPUDEAC0029_pataka.wav', 'AVPEPUDEA0058_pataka.wav',\n",
              "       'AVPEPUDEAC0049_pataka.wav', 'AVPEPUDEA0007_pataka.wav',\n",
              "       'AVPEPUDEAC0027_pataka.wav', 'AVPEPUDEAC0006_pataka.wav',\n",
              "       'AVPEPUDEA0010_pataka.wav', 'AVPEPUDEAC0057_pataka.wav',\n",
              "       'AVPEPUDEAC0024_pataka.wav', 'AVPEPUDEA0047_pataka.wav',\n",
              "       'AVPEPUDEAC0005_pataka.wav', 'AVPEPUDEA0030_pataka.wav',\n",
              "       'AVPEPUDEAC0034_pataka.wav', 'AVPEPUDEA0021_pataka.wav',\n",
              "       'AVPEPUDEA0025_pataka.wav', 'AVPEPUDEAC0048_pataka.wav',\n",
              "       'AVPEPUDEAC0053_pataka.wav', 'AVPEPUDEA0049_pataka.wav',\n",
              "       'AVPEPUDEAC0011_pataka.wav', 'AVPEPUDEA0013_pataka.wav',\n",
              "       'AVPEPUDEAC0001_pataka.wav', 'AVPEPUDEA0050_pataka.wav',\n",
              "       'AVPEPUDEA0009_pataka.wav', 'AVPEPUDEAC0025_pataka.wav',\n",
              "       'AVPEPUDEA0014_pataka.wav', 'AVPEPUDEA0016_pataka.wav',\n",
              "       'AVPEPUDEAC0010_pataka.wav', 'AVPEPUDEA0056_pataka.wav',\n",
              "       'AVPEPUDEAC0016_pataka.wav', 'AVPEPUDEA0023_pataka.wav',\n",
              "       'AVPEPUDEA0045_pataka.wav', 'AVPEPUDEAC0017_pataka.wav',\n",
              "       'AVPEPUDEAC0033_pataka.wav', 'AVPEPUDEAC0031_pataka.wav',\n",
              "       'AVPEPUDEAC0014_pataka.wav', 'AVPEPUDEAC0021_pataka.wav',\n",
              "       'AVPEPUDEAC0023_pataka.wav', 'AVPEPUDEA0001_petaka.wav',\n",
              "       'AVPEPUDEAC0028_pataka.wav', 'AVPEPUDEAC0008_pataka.wav',\n",
              "       'AVPEPUDEA0026_pataka.wav', 'AVPEPUDEA0032_pataka.wav',\n",
              "       'AVPEPUDEAC0003_pataka.wav', 'AVPEPUDEAC0019_pataka.wav',\n",
              "       'AVPEPUDEA0017_pataka.wav', 'AVPEPUDEA0037_pataka.wav',\n",
              "       'AVPEPUDEAC0007_pataka.wav', 'AVPEPUDEA0031_pataka.wav',\n",
              "       'AVPEPUDEAC0012_pataka.wav', 'AVPEPUDEA0027_pataka.wav',\n",
              "       'AVPEPUDEAC0015_pataka.wav', 'AVPEPUDEAC0043_pataka.wav'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 401
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv7Hr2lReIcs",
        "outputId": "22f21f53-68dc-478b-f013-6d3140b29aa7"
      },
      "execution_count": 402,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
              "       0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
              "       0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
              "       0, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 402
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Perform Data Augmentation**"
      ],
      "metadata": {
        "id": "lA0e_pb3Z8U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install audiomentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSHhHcb9awUv",
        "outputId": "b12c1762-c883-4e8b-bc93-5d7c4d787fe5"
      },
      "execution_count": 403,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: audiomentations in /usr/local/lib/python3.7/dist-packages (0.24.0)\n",
            "Requirement already satisfied: scipy<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (1.21.6)\n",
            "Requirement already satisfied: librosa<0.10.0,>0.7.2 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (0.8.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (1.1.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (2.1.9)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (0.51.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (0.10.3.post1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (1.0.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (0.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (21.3)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (1.6.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa<0.10.0,>0.7.2->audiomentations) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa<0.10.0,>0.7.2->audiomentations) (57.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa<0.10.0,>0.7.2->audiomentations) (3.0.8)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (3.0.4)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa<0.10.0,>0.7.2->audiomentations) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa<0.10.0,>0.7.2->audiomentations) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa<0.10.0,>0.7.2->audiomentations) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa<0.10.0,>0.7.2->audiomentations) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation was applied only to the audios in the training split, and not to the ones in the test split. These transformations include Noise Addition, Pitch Scaling (change of the pitch of the voice), Time Stretching (changing the speed of the sound but without changing the pitch) and Polarity Inversion (multiply the waveform by -1)."
      ],
      "metadata": {
        "id": "nxhYzCDvNSFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from audiomentations import Compose, AddGaussianNoise, PitchShift, TimeStretch, PolarityInversion, Normalize, HighPassFilter\n",
        "\n",
        "NUM_AUGMENTATIONS = 10 #number of augmentations per training signal\n",
        "\n",
        "augment = Compose([\n",
        "  AddGaussianNoise(min_amplitude=0.1, max_amplitude=0.2, p=0.5),\n",
        "  PitchShift(min_semitones=-4, max_semitones=4, p=0.7),\n",
        "  TimeStretch(min_rate=0.8, max_rate=1.25, p=0.7),\n",
        "  PolarityInversion(p=0.7),\n",
        "])\n"
      ],
      "metadata": {
        "id": "CCJHi9VcZ7ym"
      },
      "execution_count": 404,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Obtain MFCCs and create train and test data subsets with them**"
      ],
      "metadata": {
        "id": "veH2dSqLRqOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_RATE = 44100\n",
        "NUM_MFCC = 40\n",
        "MFCC_MAX_LEN = 500"
      ],
      "metadata": {
        "id": "Ee3UivC3LLmH"
      },
      "execution_count": 405,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_audio(audio):\n",
        "    audio = audio / np.max(np.abs(audio))\n",
        "    return audio"
      ],
      "metadata": {
        "id": "WsdOwAt_lua5"
      },
      "execution_count": 406,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to extract MFCCs from audio signal"
      ],
      "metadata": {
        "id": "ds8NpL0fLFhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def audio_to_mfcc(audio, max_len=MFCC_MAX_LEN):\n",
        "\n",
        "    audio = normalize_audio(audio)\n",
        "    \n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=SAMPLE_RATE, n_mfcc=NUM_MFCC)\n",
        "\n",
        "    # If maximum length exceeds mfcc lengths then pad the remaining ones\n",
        "    if (max_len > mfcc.shape[1]):\n",
        "        pad_width = max_len - mfcc.shape[1]\n",
        "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "\n",
        "    # Else cutoff the remaining parts\n",
        "    else:\n",
        "        mfcc = mfcc[:, :max_len]\n",
        "    \n",
        "    return mfcc"
      ],
      "metadata": {
        "id": "XtfpXpXHUfl4"
      },
      "execution_count": 407,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def append_X_Y(X, y, label, audio):\n",
        "    y.append(label)\n",
        "    mfcc = audio_to_mfcc(audio)\n",
        "    X.append(mfcc)"
      ],
      "metadata": {
        "id": "VOFlsnbYIQOl"
      },
      "execution_count": 408,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create new training dataset with the MFCC coefficients obtained from the original and augmented audios"
      ],
      "metadata": {
        "id": "_Vk4wfLtLLbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "PD_idx = 0\n",
        "HC_idx = 0\n",
        "\n",
        "for idx, audio_filename in tqdm(enumerate(df_X_train)):\n",
        "    label = df_y_train[idx]\n",
        "    audio, sr = librosa.load(TRAIN_PATH + audio_filename, sr=44100)\n",
        "    \n",
        "    if (label == 1):\n",
        "      PD_idx = idx\n",
        "    else:\n",
        "      HC_idx = idx\n",
        "\n",
        "    append_X_Y(X_train, y_train, label, audio)\n",
        "\n",
        "    for i in range(NUM_AUGMENTATIONS):\n",
        "      augmented_audio = augment(audio, sr)\n",
        "      append_X_Y(X_train, y_train, label, augmented_audio)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ex_Ks4cIQMT",
        "outputId": "f4df0a25-10ba-4ffd-8b52-a0c2fda87848"
      },
      "execution_count": 409,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "70it [04:54,  4.20s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_train.shape[0] == len(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5VBfetjIQJ3",
        "outputId": "8f3a4898-70ce-4358-cf10-f4b2fb54f126"
      },
      "execution_count": 410,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 410
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQfrTEKkv-wy",
        "outputId": "5cf6764a-ba46-44e9-eb29-7c8b8a3e5521"
      },
      "execution_count": 411,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 411
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDTecn3YdyYE",
        "outputId": "1f71b7b4-52ef-4697-b4a2-402f24fe116b"
      },
      "execution_count": 412,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 412
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random shuffle of the new dataset\n",
        "X_train, y_train = sklearn.utils.shuffle(X_train, y_train)"
      ],
      "metadata": {
        "id": "2G9qwXF4soOj"
      },
      "execution_count": 413,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLEa584Us08V",
        "outputId": "caa54fc1-1d14-4313-bf2d-879960b8072d"
      },
      "execution_count": 414,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
              "       1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
              "       1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
              "       0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
              "       0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
              "       1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
              "       1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
              "       1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
              "       1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
              "       0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
              "       1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
              "       1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
              "       0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
              "       0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
              "       1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
              "       1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
              "       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
              "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
              "       1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
              "       0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 414
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#X_val = []\n",
        "#y_val = []\n",
        "\n",
        "#for idx, audio_filename in tqdm(enumerate(df_X_val)):\n",
        "#    label = df_y_val[idx]\n",
        "#    audio, sr = librosa.load(TRAIN_PATH + audio_filename, sr=44100)\n",
        "    \n",
        "#    append_X_Y(X_val, y_val, label, audio)"
      ],
      "metadata": {
        "id": "BirA-Bz1uH3L"
      },
      "execution_count": 415,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_val = np.array(X_val)\n",
        "#y_val = np.array(y_val)\n",
        "#X_val.shape[0] == len(y_val)"
      ],
      "metadata": {
        "id": "nPiTROmourue"
      },
      "execution_count": 416,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create new test dataset with the MFCC coefficients obtained from the original audios"
      ],
      "metadata": {
        "id": "oUFedNzbLmbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "for idx, audio_filename in tqdm(enumerate(df_X_test)):\n",
        "    label = df_y_test[idx]\n",
        "    audio, sr = librosa.load(TRAIN_PATH + audio_filename, sr=44100)\n",
        "    \n",
        "    append_X_Y(X_test, y_test, label, audio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8owpUchuh5f",
        "outputId": "1af6876c-347e-446b-bd28-2f6b37c232be"
      },
      "execution_count": 417,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "30it [00:01, 19.59it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "X_test.shape[0] == len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqwJ3F0CuxB2",
        "outputId": "3e3ee3fe-63e4-44f5-b6b7-296e8578619e"
      },
      "execution_count": 418,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 418
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whXgqZ1PVVl-",
        "outputId": "def7836c-0688-47a5-d7c7-278aede1544c"
      },
      "execution_count": 419,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 419
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Build TDNN as a sequential model**"
      ],
      "metadata": {
        "id": "vWBwPbc9R0V-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_dim_1 = NUM_MFCC\n",
        "feature_dim_2 = MFCC_MAX_LEN"
      ],
      "metadata": {
        "id": "B6guoCbNYCIZ"
      },
      "execution_count": 420,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TDNN model\n",
        "numFilters = 32\n",
        "dropout_rate = 0.2\n"
      ],
      "metadata": {
        "id": "aD9Vdbd5xBlw"
      },
      "execution_count": 425,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = Input(shape=(feature_dim_1, feature_dim_2))\n",
        "\n",
        "# Layer 1\n",
        "x = Conv1D(numFilters, 5, dilation_rate=1)(input_tensor)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(dropout_rate)(x)\n",
        "x = ReLU()(x)\n",
        "# Layer 2\n",
        "x = Conv1D(numFilters, 3, dilation_rate=2)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(dropout_rate)(x)\n",
        "x = ReLU()(x)\n",
        "# Layer 3\n",
        "x = Conv1D(numFilters, 3, dilation_rate=3)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(dropout_rate)(x)\n",
        "x = ReLU()(x)\n",
        "# Layer 4\n",
        "x = Conv1D(numFilters, 1, dilation_rate=1)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(dropout_rate)(x)\n",
        "x = ReLU()(x)\n",
        "# Layer 5\n",
        "x = Conv1D(1500, 1, dilation_rate=1)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(dropout_rate)(x)\n",
        "x = ReLU()(x)\n",
        "\n",
        "# Layer 6: stats pooling\n",
        "mean = tf.math.reduce_mean(x, axis=1)\n",
        "std = tf.math.reduce_variance(x, axis=1)\n",
        "stat_pooling = tf.concat((mean, std), axis=1)\n",
        "x_vector = Activation('linear')(stat_pooling)  #x-vectors\n",
        "\n",
        "x_vec_model = Model(inputs = input_tensor, outputs = x_vector);"
      ],
      "metadata": {
        "id": "JTOF6MwvKzzr"
      },
      "execution_count": 426,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_vec_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M61S6KS_OcnO",
        "outputId": "2f536004-5257-4ece-8202-6f2733a94182"
      },
      "execution_count": 427,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)          [(None, 40, 500)]    0           []                               \n",
            "                                                                                                  \n",
            " conv1d_50 (Conv1D)             (None, 36, 32)       80032       ['input_11[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 36, 32)      128         ['conv1d_50[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_50 (Dropout)           (None, 36, 32)       0           ['batch_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " re_lu_50 (ReLU)                (None, 36, 32)       0           ['dropout_50[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_51 (Conv1D)             (None, 32, 32)       3104        ['re_lu_50[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 32, 32)      128         ['conv1d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_51 (Dropout)           (None, 32, 32)       0           ['batch_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " re_lu_51 (ReLU)                (None, 32, 32)       0           ['dropout_51[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_52 (Conv1D)             (None, 26, 32)       3104        ['re_lu_51[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_52 (BatchN  (None, 26, 32)      128         ['conv1d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_52 (Dropout)           (None, 26, 32)       0           ['batch_normalization_52[0][0]'] \n",
            "                                                                                                  \n",
            " re_lu_52 (ReLU)                (None, 26, 32)       0           ['dropout_52[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_53 (Conv1D)             (None, 26, 32)       1056        ['re_lu_52[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_53 (BatchN  (None, 26, 32)      128         ['conv1d_53[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_53 (Dropout)           (None, 26, 32)       0           ['batch_normalization_53[0][0]'] \n",
            "                                                                                                  \n",
            " re_lu_53 (ReLU)                (None, 26, 32)       0           ['dropout_53[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_54 (Conv1D)             (None, 26, 1500)     49500       ['re_lu_53[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_54 (BatchN  (None, 26, 1500)    6000        ['conv1d_54[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_54 (Dropout)           (None, 26, 1500)     0           ['batch_normalization_54[0][0]'] \n",
            "                                                                                                  \n",
            " re_lu_54 (ReLU)                (None, 26, 1500)     0           ['dropout_54[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_10 (TFOpLa  (None, 1500)        0           ['re_lu_54[0][0]']               \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.math.reduce_variance_10 (TF  (None, 1500)        0           ['re_lu_54[0][0]']               \n",
            " OpLambda)                                                                                        \n",
            "                                                                                                  \n",
            " tf.concat_10 (TFOpLambda)      (None, 3000)         0           ['tf.math.reduce_mean_10[0][0]', \n",
            "                                                                  'tf.math.reduce_variance_10[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 3000)         0           ['tf.concat_10[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 143,308\n",
            "Trainable params: 140,052\n",
            "Non-trainable params: 3,256\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUaq8hUSGDKR",
        "outputId": "6b89e313-08d5-4710-939e-1a17984cce1c"
      },
      "execution_count": 428,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 428
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The x-vectors are extracted for each of the test samples. These x-vectors will be the input to the LDA (or PLDA) model, which will be trained to fit the training data. The TDNN model is not trained as there aren't any dense layers (the model works only as and embedder to extract the x-vectors)."
      ],
      "metadata": {
        "id": "ix8TwORMLz4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_vectors_training = x_vec_model.predict(X_train)"
      ],
      "metadata": {
        "id": "MLeijDV9E2-7"
      },
      "execution_count": 429,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_vectors_training.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "919ynnP7FJZr",
        "outputId": "09f01899-312c-4bb4-aef5-0b68d1f12957"
      },
      "execution_count": 430,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(770, 3000)"
            ]
          },
          "metadata": {},
          "execution_count": 430
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#x_vectors_val = x_vec_model.predict(X_val)"
      ],
      "metadata": {
        "id": "7Mtv-voUG2Jf"
      },
      "execution_count": 431,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#x_vectors_val.shape"
      ],
      "metadata": {
        "id": "7oknER0jG8ov"
      },
      "execution_count": 432,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_vectors_test = x_vec_model.predict(X_test)"
      ],
      "metadata": {
        "id": "TqTI5UOOG_HV"
      },
      "execution_count": 433,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_vectors_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBuAiF_THDj1",
        "outputId": "d81b1649-ccdc-444d-ca6d-926bcd9d9e6a"
      },
      "execution_count": 434,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 3000)"
            ]
          },
          "metadata": {},
          "execution_count": 434
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV6ype7cEKp_",
        "outputId": "fa006767-fe90-4b59-d776-69ec3298a8fa"
      },
      "execution_count": 435,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: packaging<22.0,>=0.21 in /usr/local/lib/python3.7/dist-packages (from scikeras) (21.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikeras) (1.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=3 in /usr/local/lib/python3.7/dist-packages (from scikeras) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3->scikeras) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3->scikeras) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging<22.0,>=0.21->scikeras) (3.0.8)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train LDA model**"
      ],
      "metadata": {
        "id": "Pwk10JKvZMJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of this model will be the classifications of the audios (1 for PD and 0 for HC) embedded as x-vectors. "
      ],
      "metadata": {
        "id": "64CzaTIzMl_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_validate, cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#epochs = 20\n",
        "#batch_size = 5\n",
        "\n",
        "# LDA model definition\n",
        "LDA_model = LinearDiscriminantAnalysis()\n",
        "\n",
        "# Create pipeline\n",
        "#pipeline = Pipeline(steps=[('x_vec_model',x_vec),('LDA_model',LDA_model)])\n",
        "\n",
        "cv_results = cross_validate(LDA_model, x_vectors_training, y_train, cv=5, \n",
        "                            scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\", \"roc_auc\"], return_train_score=True, verbose=1)\n",
        "\n",
        "print(\"\\nValidation accuracy: {mean_accuracy} +/- {std_accuracy}\".format(\n",
        "    mean_accuracy=np.mean(cv_results['test_accuracy']),\n",
        "    std_accuracy=np.std(cv_results['test_accuracy'])))\n",
        "print(\"Training accuracy: {mean_accuracy} +/- {std_accuracy}\".format(\n",
        "    mean_accuracy=np.mean(cv_results['train_accuracy']),\n",
        "    std_accuracy=np.std(cv_results['train_accuracy'])))\n",
        "print(\"Validation recall: {mean_recall} +/- {std_recall}\".format(\n",
        "    mean_recall=np.mean(cv_results['test_recall_macro']),\n",
        "    std_recall=np.std(cv_results['test_recall_macro'])))\n",
        "print(\"Training recall: {mean_recall} +/- {std_recall}\".format(\n",
        "    mean_recall=np.mean(cv_results['train_recall_macro']),\n",
        "    std_recall=np.std(cv_results['train_recall_macro'])))\n",
        "print(\"Validation precision: {mean_precision} +/- {std_precision}\".format(\n",
        "    mean_precision=np.mean(cv_results['test_precision_macro']),\n",
        "    std_precision=np.std(cv_results['test_precision_macro'])))\n",
        "print(\"Training precision: {mean_precision} +/- {std_precision} \\n\".format(\n",
        "    mean_precision=np.mean(cv_results['train_precision_macro']),\n",
        "    std_precision=np.std(cv_results['train_precision_macro'])))\n",
        "\n",
        "print(\"Validation ROC AUC: {mean_auc} +/- {std_auc}\".format(\n",
        "    mean_auc=np.mean(cv_results['test_roc_auc']),\n",
        "    std_auc=np.std(cv_results['test_roc_auc'])))\n",
        "print(\"Training ROC AUC: {mean_auc} +/- {std_auc} \\n\".format(\n",
        "    mean_auc=np.mean(cv_results['train_roc_auc']),\n",
        "    std_auc=np.std(cv_results['train_roc_auc'])))\n",
        "\n",
        "print(\"Fit mean time: {fit_time}\".format(fit_time=np.mean(cv_results['fit_time'])))\n",
        "print(\"Score mean time: {score_time} \\n\".format(score_time=np.mean(cv_results['score_time'])))\n",
        "\n",
        "#LDA_acc = np.mean(cv_results['test_accuracy'])\n",
        "#LDA_auc = np.mean(cv_results['test_roc_auc'])\n",
        "\n",
        "# Confusion matrix\n",
        "y_pred = cross_val_predict(LDA_model, x_vectors_test, y_test, cv=5)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm_results = {'tn': cm[0, 0], 'fp': cm[0, 1], 'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"True Positives: {tp}\".format(tp=cm_results['tp']))\n",
        "print(\"False Positives: {fp}\".format(fp=cm_results['fp']))\n",
        "print(\"True Negatives: {tn}\".format(tn=cm_results['tn']))\n",
        "print(\"False Negatives: {fn}\\n\".format(fn=cm_results['fn']))\n",
        "\n",
        "test_acc = (cm_results['tp']+cm_results['tn'])/(cm_results['tp']+cm_results['fp']+cm_results['tn']+cm_results['fn'])\n",
        "test_precision = cm_results['tp']/(cm_results['tp']+cm_results['fp'])\n",
        "test_recall = cm_results['tp']/(cm_results['tp']+cm_results['fn'])\n",
        "\n",
        "print(\"Test accuracy: {}\".format(test_acc))\n",
        "print(\"Test precision: {}\".format(test_precision))\n",
        "print(\"Test recall: {}\".format(test_recall))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1odTqyh7TAz",
        "outputId": "e18718ef-505e-417b-a304-7a662c582588"
      },
      "execution_count": 436,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   11.1s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation accuracy: 0.5506493506493506 +/- 0.02373463231493125\n",
            "Training accuracy: 0.9594155844155845 +/- 0.00543285731515633\n",
            "Validation recall: 0.5502018189075152 +/- 0.022446071701890693\n",
            "Training recall: 0.9593719230571132 +/- 0.005414982516695803\n",
            "Validation precision: 0.5512306015225297 +/- 0.02360519878886672\n",
            "Training precision: 0.9594045304865293 +/- 0.005465748674052066 \n",
            "\n",
            "Validation ROC AUC: 0.5531023206751055 +/- 0.034929428665732405\n",
            "Training ROC AUC: 0.983265516088248 +/- 0.0015443565731601673 \n",
            "\n",
            "Fit mean time: 2.1630436420440673\n",
            "Score mean time: 0.017424821853637695 \n",
            "\n",
            "Confusion Matrix:\n",
            "True Positives: 11\n",
            "False Positives: 6\n",
            "True Negatives: 8\n",
            "False Negatives: 5\n",
            "\n",
            "Test accuracy: 0.6333333333333333\n",
            "Test precision: 0.6470588235294118\n",
            "Test recall: 0.6875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute only for cosine similarity measure (finally not applied)"
      ],
      "metadata": {
        "id": "2otO9dOO7Nxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print(tf.shape(x_vector))\n",
        "#ref_xvector_PD = np.zeros([3000], dtype=np.float32)\n",
        "#ref_xvector_PD = tf.convert_to_tensor(ref_xvector_PD)\n",
        "#ref_xvector_HC = np.zeros([3000], dtype=np.float32)\n",
        "#ref_xvector_HC = tf.convert_to_tensor(ref_xvector_HC)\n",
        "\n",
        "#x_vector_norm = K.l2_normalize(x_vector, axis=-1)\n",
        "#ref_xvector_PD_norm = K.l2_normalize(ref_xvector_PD, axis=-1)\n",
        "#ref_xvector_HC_norm = K.l2_normalize(ref_xvector_HC, axis=-1)\n",
        "#similarity_PD = K.batch_dot(x_vector_norm, ref_xvector_PD_norm, axes=-1)\n",
        "#similarity_HC = K.batch_dot(x_vector_norm, ref_xvector_HC_norm, axes=-1)\n",
        "\n",
        "#result = tf.subtract(similarity_PD, similarity_HC)\n",
        "\n",
        "#prediction = Dense(1, activation='softmax')(result)\n",
        "\n",
        "#model = Model(inputs = input_tensor, outputs = result);"
      ],
      "metadata": {
        "id": "3BAxFB9L7G3v"
      },
      "execution_count": 438,
      "outputs": []
    }
  ]
}