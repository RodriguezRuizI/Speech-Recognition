{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YnC9nFpl_uP_",
    "outputId": "cf75aea7-a3f2-4058-98dd-6d36be4f557b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras import Sequential, Input, Model\n",
    "from keras.layers import Conv1D, BatchNormalization, Dropout, Dense, Softmax, ReLU, Lambda, Activation\n",
    "from keras import optimizers\n",
    "\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed9DcgobRk3W",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Load data into Pandas dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vFQVZEo6Hk-8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "useGoogleColab = True\n",
    "\n",
    "if useGoogleColab:\n",
    "    # Connect Google Colab to Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive/')\n",
    "    TRAIN_PATH = '/content/gdrive/My Drive/Gita/Pataka/'\n",
    "    dataset_file = TRAIN_PATH + 'labels.csv'\n",
    "else:\n",
    "    TRAIN_PATH = './Pataka/'\n",
    "    dataset_file = 'labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "4e3spzBqIPf1",
    "outputId": "33ed3eab-bcdb-4431-ff93-de3280ecf94b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AVPEPUDEAC0001_pataka.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AVPEPUDEAC0003_pataka.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AVPEPUDEAC0004_pataka.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AVPEPUDEAC0005_pataka.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AVPEPUDEAC0006_pataka.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>AVPEPUDEA0055_pataka.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>AVPEPUDEA0056_pataka.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>AVPEPUDEA0057_pataka.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>AVPEPUDEA0058_pataka.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>AVPEPUDEA0059_pataka.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                   Filename  Gender  Disease\n",
       "0            0  AVPEPUDEAC0001_pataka.wav     NaN        0\n",
       "1            1  AVPEPUDEAC0003_pataka.wav     NaN        0\n",
       "2            2  AVPEPUDEAC0004_pataka.wav     NaN        0\n",
       "3            3  AVPEPUDEAC0005_pataka.wav     NaN        0\n",
       "4            4  AVPEPUDEAC0006_pataka.wav     NaN        0\n",
       "..         ...                        ...     ...      ...\n",
       "95          95   AVPEPUDEA0055_pataka.wav     NaN        1\n",
       "96          96   AVPEPUDEA0056_pataka.wav     NaN        1\n",
       "97          97   AVPEPUDEA0057_pataka.wav     NaN        1\n",
       "98          98   AVPEPUDEA0058_pataka.wav     NaN        1\n",
       "99          99   AVPEPUDEA0059_pataka.wav     NaN        1\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(dataset_file)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "id": "5odmPDl_Qu-o",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(data[\"Filename\"].to_numpy(), data[\"Disease\"].to_numpy(), test_size= 0.3, random_state=True, shuffle=True)\n",
    "#df_X_train, df_X_val, df_y_train, df_y_val = train_test_split(df_X_train, df_y_train, test_size=0.2, random_state=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Fn4C78Hvk7g",
    "outputId": "c7d60672-ea50-418b-d76c-ede9cbae8173",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AVPEPUDEA0022_pataka.wav', 'AVPEPUDEA0059_pataka.wav',\n",
       "       'AVPEPUDEA0006_pataka.wav', 'AVPEPUDEA0055_pataka.wav',\n",
       "       'AVPEPUDEA0048_pataka.wav', 'AVPEPUDEAC0046_pataka.wav',\n",
       "       'AVPEPUDEAC0054_pataka.wav', 'AVPEPUDEA0011_pataka.wav',\n",
       "       'AVPEPUDEAC0026_pataka.wav', 'AVPEPUDEAC0040_pataka.wav',\n",
       "       'AVPEPUDEA0046_pataka.wav', 'AVPEPUDEA0005_pataka.wav',\n",
       "       'AVPEPUDEA0034_pataka.wav', 'AVPEPUDEAC0018_pataka.wav',\n",
       "       'AVPEPUDEA0042_pataka.wav', 'AVPEPUDEAC0047_pataka.wav',\n",
       "       'AVPEPUDEAC0051_pataka.wav', 'AVPEPUDEA0051_pataka.wav',\n",
       "       'AVPEPUDEAC0029_pataka.wav', 'AVPEPUDEA0058_pataka.wav',\n",
       "       'AVPEPUDEAC0049_pataka.wav', 'AVPEPUDEA0007_pataka.wav',\n",
       "       'AVPEPUDEAC0027_pataka.wav', 'AVPEPUDEAC0006_pataka.wav',\n",
       "       'AVPEPUDEA0010_pataka.wav', 'AVPEPUDEAC0057_pataka.wav',\n",
       "       'AVPEPUDEAC0024_pataka.wav', 'AVPEPUDEA0047_pataka.wav',\n",
       "       'AVPEPUDEAC0005_pataka.wav', 'AVPEPUDEA0030_pataka.wav',\n",
       "       'AVPEPUDEAC0034_pataka.wav', 'AVPEPUDEA0021_pataka.wav',\n",
       "       'AVPEPUDEA0025_pataka.wav', 'AVPEPUDEAC0048_pataka.wav',\n",
       "       'AVPEPUDEAC0053_pataka.wav', 'AVPEPUDEA0049_pataka.wav',\n",
       "       'AVPEPUDEAC0011_pataka.wav', 'AVPEPUDEA0013_pataka.wav',\n",
       "       'AVPEPUDEAC0001_pataka.wav', 'AVPEPUDEA0050_pataka.wav',\n",
       "       'AVPEPUDEA0009_pataka.wav', 'AVPEPUDEAC0025_pataka.wav',\n",
       "       'AVPEPUDEA0014_pataka.wav', 'AVPEPUDEA0016_pataka.wav',\n",
       "       'AVPEPUDEAC0010_pataka.wav', 'AVPEPUDEA0056_pataka.wav',\n",
       "       'AVPEPUDEAC0016_pataka.wav', 'AVPEPUDEA0023_pataka.wav',\n",
       "       'AVPEPUDEA0045_pataka.wav', 'AVPEPUDEAC0017_pataka.wav',\n",
       "       'AVPEPUDEAC0033_pataka.wav', 'AVPEPUDEAC0031_pataka.wav',\n",
       "       'AVPEPUDEAC0014_pataka.wav', 'AVPEPUDEAC0021_pataka.wav',\n",
       "       'AVPEPUDEAC0023_pataka.wav', 'AVPEPUDEA0001_petaka.wav',\n",
       "       'AVPEPUDEAC0028_pataka.wav', 'AVPEPUDEAC0008_pataka.wav',\n",
       "       'AVPEPUDEA0026_pataka.wav', 'AVPEPUDEA0032_pataka.wav',\n",
       "       'AVPEPUDEAC0003_pataka.wav', 'AVPEPUDEAC0019_pataka.wav',\n",
       "       'AVPEPUDEA0017_pataka.wav', 'AVPEPUDEA0037_pataka.wav',\n",
       "       'AVPEPUDEAC0007_pataka.wav', 'AVPEPUDEA0031_pataka.wav',\n",
       "       'AVPEPUDEAC0012_pataka.wav', 'AVPEPUDEA0027_pataka.wav',\n",
       "       'AVPEPUDEAC0015_pataka.wav', 'AVPEPUDEAC0043_pataka.wav'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xv7Hr2lReIcs",
    "outputId": "22f21f53-68dc-478b-f013-6d3140b29aa7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0])"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lA0e_pb3Z8U-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Perform Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bSHhHcb9awUv",
    "outputId": "b12c1762-c883-4e8b-bc93-5d7c4d787fe5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: audiomentations in /usr/local/lib/python3.7/dist-packages (0.24.0)\n",
      "Requirement already satisfied: scipy<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (1.21.6)\n",
      "Requirement already satisfied: librosa<0.10.0,>0.7.2 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (0.8.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (1.1.0)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (2.1.9)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (4.4.2)\n",
      "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (0.51.2)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (0.10.3.post1)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (1.0.2)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (0.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (21.3)\n",
      "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations) (1.6.0)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa<0.10.0,>0.7.2->audiomentations) (0.34.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa<0.10.0,>0.7.2->audiomentations) (57.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa<0.10.0,>0.7.2->audiomentations) (3.0.8)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (2.23.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations) (3.0.4)\n",
      "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa<0.10.0,>0.7.2->audiomentations) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa<0.10.0,>0.7.2->audiomentations) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa<0.10.0,>0.7.2->audiomentations) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa<0.10.0,>0.7.2->audiomentations) (2.21)\n"
     ]
    }
   ],
   "source": [
    "%pip install audiomentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxhYzCDvNSFH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Data augmentation was applied only to the audios in the training split, and not to the ones in the test split. These transformations include Noise Addition, Pitch Scaling (change of the pitch of the voice), Time Stretching (changing the speed of the sound but without changing the pitch) and Polarity Inversion (multiply the waveform by -1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CCJHi9VcZ7ym",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from audiomentations import Compose, AddGaussianNoise, PitchShift, TimeStretch, PolarityInversion, Normalize, HighPassFilter\n",
    "\n",
    "NUM_AUGMENTATIONS = 10 #number of augmentations per training signal\n",
    "\n",
    "augment = Compose([\n",
    "  AddGaussianNoise(min_amplitude=0.1, max_amplitude=0.2, p=0.5),\n",
    "  PitchShift(min_semitones=-4, max_semitones=4, p=0.7),\n",
    "  TimeStretch(min_rate=0.8, max_rate=1.25, p=0.7),\n",
    "  PolarityInversion(p=0.7),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veH2dSqLRqOZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Obtain MFCCs and create train and test data subsets with them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "id": "Ee3UivC3LLmH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 44100\n",
    "NUM_MFCC = 40\n",
    "MFCC_MAX_LEN = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "id": "WsdOwAt_lua5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_audio(audio):\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ds8NpL0fLFhd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Function to extract MFCCs from audio signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "id": "XtfpXpXHUfl4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def audio_to_mfcc(audio, max_len=MFCC_MAX_LEN):\n",
    "\n",
    "    audio = normalize_audio(audio)\n",
    "    \n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=SAMPLE_RATE, n_mfcc=NUM_MFCC)\n",
    "\n",
    "    # If maximum length exceeds mfcc lengths then pad the remaining ones\n",
    "    if (max_len > mfcc.shape[1]):\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    # Else cutoff the remaining parts\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    \n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "id": "VOFlsnbYIQOl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def append_X_Y(X, y, label, audio):\n",
    "    y.append(label)\n",
    "    mfcc = audio_to_mfcc(audio)\n",
    "    X.append(mfcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Vk4wfLtLLbu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create new training dataset with the MFCC coefficients obtained from the original and augmented audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Ex_Ks4cIQMT",
    "outputId": "f4df0a25-10ba-4ffd-8b52-a0c2fda87848",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [04:54,  4.20s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "PD_idx = 0\n",
    "HC_idx = 0\n",
    "\n",
    "for idx, audio_filename in tqdm(enumerate(df_X_train)):\n",
    "    label = df_y_train[idx]\n",
    "    audio, sr = librosa.load(TRAIN_PATH + audio_filename, sr=44100)\n",
    "    \n",
    "    if (label == 1):\n",
    "      PD_idx = idx\n",
    "    else:\n",
    "      HC_idx = idx\n",
    "\n",
    "    append_X_Y(X_train, y_train, label, audio)\n",
    "\n",
    "    for i in range(NUM_AUGMENTATIONS):\n",
    "      augmented_audio = augment(audio, sr)\n",
    "      append_X_Y(X_train, y_train, label, augmented_audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5VBfetjIQJ3",
    "outputId": "8f3a4898-70ce-4358-cf10-f4b2fb54f126",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_train.shape[0] == len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQfrTEKkv-wy",
    "outputId": "5cf6764a-ba46-44e9-eb29-7c8b8a3e5521",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 500)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDTecn3YdyYE",
    "outputId": "1f71b7b4-52ef-4697-b4a2-402f24fe116b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "id": "2G9qwXF4soOj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Random shuffle of the new dataset\n",
    "X_train, y_train = sklearn.utils.shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLEa584Us08V",
    "outputId": "caa54fc1-1d14-4313-bf2d-879960b8072d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "id": "BirA-Bz1uH3L",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#X_val = []\n",
    "#y_val = []\n",
    "\n",
    "#for idx, audio_filename in tqdm(enumerate(df_X_val)):\n",
    "#    label = df_y_val[idx]\n",
    "#    audio, sr = librosa.load(TRAIN_PATH + audio_filename, sr=44100)\n",
    "    \n",
    "#    append_X_Y(X_val, y_val, label, audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "id": "nPiTROmourue",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#X_val = np.array(X_val)\n",
    "#y_val = np.array(y_val)\n",
    "#X_val.shape[0] == len(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUFedNzbLmbw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create new test dataset with the MFCC coefficients obtained from the original audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S8owpUchuh5f",
    "outputId": "1af6876c-347e-446b-bd28-2f6b37c232be",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:01, 19.59it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for idx, audio_filename in tqdm(enumerate(df_X_test)):\n",
    "    label = df_y_test[idx]\n",
    "    audio, sr = librosa.load(TRAIN_PATH + audio_filename, sr=44100)\n",
    "    \n",
    "    append_X_Y(X_test, y_test, label, audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqwJ3F0CuxB2",
    "outputId": "3e3ee3fe-63e4-44f5-b6b7-296e8578619e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "X_test.shape[0] == len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whXgqZ1PVVl-",
    "outputId": "def7836c-0688-47a5-d7c7-278aede1544c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 500)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWBwPbc9R0V-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Build TDNN as a sequential model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "id": "B6guoCbNYCIZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_dim_1 = NUM_MFCC\n",
    "feature_dim_2 = MFCC_MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "id": "aD9Vdbd5xBlw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TDNN model\n",
    "numFilters = 32\n",
    "dropout_rate = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "id": "JTOF6MwvKzzr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(feature_dim_1, feature_dim_2))\n",
    "\n",
    "# Layer 1\n",
    "x = Conv1D(numFilters, 5, dilation_rate=1)(input_tensor)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = ReLU()(x)\n",
    "# Layer 2\n",
    "x = Conv1D(numFilters, 3, dilation_rate=2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = ReLU()(x)\n",
    "# Layer 3\n",
    "x = Conv1D(numFilters, 3, dilation_rate=3)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = ReLU()(x)\n",
    "# Layer 4\n",
    "x = Conv1D(numFilters, 1, dilation_rate=1)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = ReLU()(x)\n",
    "# Layer 5\n",
    "x = Conv1D(1500, 1, dilation_rate=1)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = ReLU()(x)\n",
    "\n",
    "# Layer 6: stats pooling\n",
    "mean = tf.math.reduce_mean(x, axis=1)\n",
    "std = tf.math.reduce_variance(x, axis=1)\n",
    "stat_pooling = tf.concat((mean, std), axis=1)\n",
    "x_vector = Activation('linear')(stat_pooling)  #x-vectors\n",
    "\n",
    "x_vec_model = Model(inputs = input_tensor, outputs = x_vector);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M61S6KS_OcnO",
    "outputId": "2f536004-5257-4ece-8202-6f2733a94182",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 40, 500)]    0           []                               \n",
      "                                                                                                  \n",
      " conv1d_50 (Conv1D)             (None, 36, 32)       80032       ['input_11[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 36, 32)      128         ['conv1d_50[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_50 (Dropout)           (None, 36, 32)       0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_50 (ReLU)                (None, 36, 32)       0           ['dropout_50[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_51 (Conv1D)             (None, 32, 32)       3104        ['re_lu_50[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 32, 32)      128         ['conv1d_51[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_51 (Dropout)           (None, 32, 32)       0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_51 (ReLU)                (None, 32, 32)       0           ['dropout_51[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_52 (Conv1D)             (None, 26, 32)       3104        ['re_lu_51[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 26, 32)      128         ['conv1d_52[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_52 (Dropout)           (None, 26, 32)       0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_52 (ReLU)                (None, 26, 32)       0           ['dropout_52[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_53 (Conv1D)             (None, 26, 32)       1056        ['re_lu_52[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 26, 32)      128         ['conv1d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_53 (Dropout)           (None, 26, 32)       0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_53 (ReLU)                (None, 26, 32)       0           ['dropout_53[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_54 (Conv1D)             (None, 26, 1500)     49500       ['re_lu_53[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 26, 1500)    6000        ['conv1d_54[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_54 (Dropout)           (None, 26, 1500)     0           ['batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " re_lu_54 (ReLU)                (None, 26, 1500)     0           ['dropout_54[0][0]']             \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_10 (TFOpLa  (None, 1500)        0           ['re_lu_54[0][0]']               \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.reduce_variance_10 (TF  (None, 1500)        0           ['re_lu_54[0][0]']               \n",
      " OpLambda)                                                                                        \n",
      "                                                                                                  \n",
      " tf.concat_10 (TFOpLambda)      (None, 3000)         0           ['tf.math.reduce_mean_10[0][0]', \n",
      "                                                                  'tf.math.reduce_variance_10[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 3000)         0           ['tf.concat_10[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 143,308\n",
      "Trainable params: 140,052\n",
      "Non-trainable params: 3,256\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_vec_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DUaq8hUSGDKR",
    "outputId": "6b89e313-08d5-4710-939e-1a17984cce1c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 500)"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix8TwORMLz4S",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The x-vectors are extracted for each of the test samples. These x-vectors will be the input to the LDA (or PLDA) model, which will be trained to fit the training data. The TDNN model is not trained as there aren't any dense layers (the model works only as and embedder to extract the x-vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "id": "MLeijDV9E2-7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_vectors_training = x_vec_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "919ynnP7FJZr",
    "outputId": "09f01899-312c-4bb4-aef5-0b68d1f12957",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(770, 3000)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_vectors_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "id": "7Mtv-voUG2Jf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#x_vectors_val = x_vec_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "id": "7oknER0jG8ov",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#x_vectors_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "id": "TqTI5UOOG_HV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_vectors_test = x_vec_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBuAiF_THDj1",
    "outputId": "d81b1649-ccdc-444d-ca6d-926bcd9d9e6a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 3000)"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_vectors_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dV6ype7cEKp_",
    "outputId": "fa006767-fe90-4b59-d776-69ec3298a8fa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikeras in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
      "Requirement already satisfied: packaging<22.0,>=0.21 in /usr/local/lib/python3.7/dist-packages (from scikeras) (21.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikeras) (1.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=3 in /usr/local/lib/python3.7/dist-packages (from scikeras) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3->scikeras) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3->scikeras) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging<22.0,>=0.21->scikeras) (3.0.8)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.21.6)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pwk10JKvZMJp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Train LDA model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64CzaTIzMl_X",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The output of this model will be the classifications of the audios (1 for PD and 0 for HC) embedded as x-vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K1odTqyh7TAz",
    "outputId": "e18718ef-505e-417b-a304-7a662c582588",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   11.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 0.5506493506493506 +/- 0.02373463231493125\n",
      "Training accuracy: 0.9594155844155845 +/- 0.00543285731515633\n",
      "Validation recall: 0.5502018189075152 +/- 0.022446071701890693\n",
      "Training recall: 0.9593719230571132 +/- 0.005414982516695803\n",
      "Validation precision: 0.5512306015225297 +/- 0.02360519878886672\n",
      "Training precision: 0.9594045304865293 +/- 0.005465748674052066 \n",
      "\n",
      "Validation ROC AUC: 0.5531023206751055 +/- 0.034929428665732405\n",
      "Training ROC AUC: 0.983265516088248 +/- 0.0015443565731601673 \n",
      "\n",
      "Fit mean time: 2.1630436420440673\n",
      "Score mean time: 0.017424821853637695 \n",
      "\n",
      "Confusion Matrix:\n",
      "True Positives: 11\n",
      "False Positives: 6\n",
      "True Negatives: 8\n",
      "False Negatives: 5\n",
      "\n",
      "Test accuracy: 0.6333333333333333\n",
      "Test precision: 0.6470588235294118\n",
      "Test recall: 0.6875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#epochs = 20\n",
    "#batch_size = 5\n",
    "\n",
    "# LDA model definition\n",
    "LDA_model = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Create pipeline\n",
    "#pipeline = Pipeline(steps=[('x_vec_model',x_vec),('LDA_model',LDA_model)])\n",
    "\n",
    "cv_results = cross_validate(LDA_model, x_vectors_training, y_train, cv=5, \n",
    "                            scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\", \"roc_auc\"], return_train_score=True, verbose=1)\n",
    "\n",
    "print(\"\\nValidation accuracy: {mean_accuracy} +/- {std_accuracy}\".format(\n",
    "    mean_accuracy=np.mean(cv_results['test_accuracy']),\n",
    "    std_accuracy=np.std(cv_results['test_accuracy'])))\n",
    "print(\"Training accuracy: {mean_accuracy} +/- {std_accuracy}\".format(\n",
    "    mean_accuracy=np.mean(cv_results['train_accuracy']),\n",
    "    std_accuracy=np.std(cv_results['train_accuracy'])))\n",
    "print(\"Validation recall: {mean_recall} +/- {std_recall}\".format(\n",
    "    mean_recall=np.mean(cv_results['test_recall_macro']),\n",
    "    std_recall=np.std(cv_results['test_recall_macro'])))\n",
    "print(\"Training recall: {mean_recall} +/- {std_recall}\".format(\n",
    "    mean_recall=np.mean(cv_results['train_recall_macro']),\n",
    "    std_recall=np.std(cv_results['train_recall_macro'])))\n",
    "print(\"Validation precision: {mean_precision} +/- {std_precision}\".format(\n",
    "    mean_precision=np.mean(cv_results['test_precision_macro']),\n",
    "    std_precision=np.std(cv_results['test_precision_macro'])))\n",
    "print(\"Training precision: {mean_precision} +/- {std_precision} \\n\".format(\n",
    "    mean_precision=np.mean(cv_results['train_precision_macro']),\n",
    "    std_precision=np.std(cv_results['train_precision_macro'])))\n",
    "\n",
    "print(\"Validation ROC AUC: {mean_auc} +/- {std_auc}\".format(\n",
    "    mean_auc=np.mean(cv_results['test_roc_auc']),\n",
    "    std_auc=np.std(cv_results['test_roc_auc'])))\n",
    "print(\"Training ROC AUC: {mean_auc} +/- {std_auc} \\n\".format(\n",
    "    mean_auc=np.mean(cv_results['train_roc_auc']),\n",
    "    std_auc=np.std(cv_results['train_roc_auc'])))\n",
    "\n",
    "print(\"Fit mean time: {fit_time}\".format(fit_time=np.mean(cv_results['fit_time'])))\n",
    "print(\"Score mean time: {score_time} \\n\".format(score_time=np.mean(cv_results['score_time'])))\n",
    "\n",
    "#LDA_acc = np.mean(cv_results['test_accuracy'])\n",
    "#LDA_auc = np.mean(cv_results['test_roc_auc'])\n",
    "\n",
    "# Confusion matrix\n",
    "y_pred = cross_val_predict(LDA_model, x_vectors_test, y_test, cv=5)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_results = {'tn': cm[0, 0], 'fp': cm[0, 1], 'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"True Positives: {tp}\".format(tp=cm_results['tp']))\n",
    "print(\"False Positives: {fp}\".format(fp=cm_results['fp']))\n",
    "print(\"True Negatives: {tn}\".format(tn=cm_results['tn']))\n",
    "print(\"False Negatives: {fn}\\n\".format(fn=cm_results['fn']))\n",
    "\n",
    "test_acc = (cm_results['tp']+cm_results['tn'])/(cm_results['tp']+cm_results['fp']+cm_results['tn']+cm_results['fn'])\n",
    "test_precision = cm_results['tp']/(cm_results['tp']+cm_results['fp'])\n",
    "test_recall = cm_results['tp']/(cm_results['tp']+cm_results['fn'])\n",
    "\n",
    "print(\"Test accuracy: {}\".format(test_acc))\n",
    "print(\"Test precision: {}\".format(test_precision))\n",
    "print(\"Test recall: {}\".format(test_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2otO9dOO7Nxc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Execute only for cosine similarity measure (finally not applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "id": "3BAxFB9L7G3v",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print(tf.shape(x_vector))\n",
    "#ref_xvector_PD = np.zeros([3000], dtype=np.float32)\n",
    "#ref_xvector_PD = tf.convert_to_tensor(ref_xvector_PD)\n",
    "#ref_xvector_HC = np.zeros([3000], dtype=np.float32)\n",
    "#ref_xvector_HC = tf.convert_to_tensor(ref_xvector_HC)\n",
    "\n",
    "#x_vector_norm = K.l2_normalize(x_vector, axis=-1)\n",
    "#ref_xvector_PD_norm = K.l2_normalize(ref_xvector_PD, axis=-1)\n",
    "#ref_xvector_HC_norm = K.l2_normalize(ref_xvector_HC, axis=-1)\n",
    "#similarity_PD = K.batch_dot(x_vector_norm, ref_xvector_PD_norm, axes=-1)\n",
    "#similarity_HC = K.batch_dot(x_vector_norm, ref_xvector_HC_norm, axes=-1)\n",
    "\n",
    "#result = tf.subtract(similarity_PD, similarity_HC)\n",
    "\n",
    "#prediction = Dense(1, activation='softmax')(result)\n",
    "\n",
    "#model = Model(inputs = input_tensor, outputs = result);"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DLAS_Xvector_model_with_DataAugmentation.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "05d51072ca50ff3329d9df1e81eea5e1b49c1cf017763d22b1e40fabe6e552d0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
